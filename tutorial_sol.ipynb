{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Let's get started\n",
    "## Importing some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_digit(img):\n",
    "    \"\"\"\n",
    "    Display the vector as a 28*28 grayscale image.\n",
    "    \"\"\"\n",
    "    img = img.reshape((28,28))\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectoried_generate_onehot_vector(labels):\n",
    "    \"\"\"\n",
    "    Returns the onehot encoding of the labels. This is a vectorized version of the function.\n",
    "    \"\"\"\n",
    "    onehot = np.zeros((len(labels), 10))\n",
    "    onehot[np.arange(len(labels)), labels] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onehot_vector(labels):\n",
    "    \"\"\"\n",
    "    Returns the onehot encoding of the labels.\n",
    "    \"\"\"\n",
    "    num_samples = len(labels)\n",
    "    onehot = np.zeros((num_samples, 10))\n",
    "    for i in range(num_samples):\n",
    "        label = labels[i]\n",
    "        onehot[i, label] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training set and testing set\n",
    "The dataset we are working with in this tutorial is the MNIST handwritten digit dataset.\n",
    "There are 60000 training examples and 10000 testing examples. We will only use 20000 of the trainig examples and 2000 of the testing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"X_train.npy\")[:20000,:]\n",
    "y_train = np.load(\"y_train.npy\")[:20000]\n",
    "X_test = np.load(\"X_test.npy\")[:2000,:]\n",
    "y_test = np.load(\"y_test.npy\")[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first trainig sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_digit(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [sklearn.preprocessing.StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to normalize the data.\n",
    "Try what would happen if you don't normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\qin\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the labels to one hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot = generate_onehot_vector(y_train)\n",
    "y_test_onehot = vectoried_generate_onehot_vector(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Setting up the neural network\n",
    "\n",
    "In this tutorial, we will use a neural network with three fully connected hidden layers. We will explore how different hyperparamter choices affect the performance of our neural network. Let's first set up some hyperparameters for our net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "iterations = 1000\n",
    "layer_1_nodes = 150\n",
    "layer_2_nodes = 150\n",
    "layer_3_nodes = 150\n",
    "layer_4_nodes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up neural network\n",
    "tf.placeholder is a variable to which we will assign values to at the later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the layers of our neural net to our [\"Computational Graph\"]([https://www.tensorflow.org/programmers_guide/graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.random_normal(shape=[784, layer_1_nodes], \n",
    "                                           dtype=tf.float32,\n",
    "                                           stddev=1e-1),\n",
    "                                           name=\"weights1\")\n",
    "b1 = tf.Variable(tf.constant(0.0, shape=[layer_1_nodes], dtype=tf.float32), name=\"biases1\")\n",
    "\n",
    "layer_1_output = tf.nn.relu(tf.matmul(X, w1) + b1)\n",
    "\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal(shape=[layer_1_nodes, layer_2_nodes], \n",
    "                                           dtype=tf.float32,\n",
    "                                           stddev=1e-1),\n",
    "                                           name=\"weights2\")\n",
    "b2 = tf.Variable(tf.constant(0.0, shape=[layer_2_nodes], dtype=tf.float32), name=\"biases2\")\n",
    "\n",
    "layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, w2) + b2)\n",
    "\n",
    "\n",
    "w3 = tf.Variable(tf.random_normal(shape=[layer_2_nodes, layer_3_nodes], \n",
    "                                           dtype=tf.float32,\n",
    "                                           stddev=1e-1),\n",
    "                                           name=\"weights3\")\n",
    "b3 = tf.Variable(tf.constant(0.0, shape=[layer_3_nodes], dtype=tf.float32), name=\"biases3\")\n",
    "\n",
    "layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, w3) + b3)\n",
    "\n",
    "w4 = tf.Variable(tf.random_normal(shape=[layer_3_nodes, layer_4_nodes], \n",
    "                                           dtype=tf.float32,\n",
    "                                           stddev=1e-1),\n",
    "                                           name=\"weights4\")\n",
    "b4 = tf.Variable(tf.constant(0.0, shape=[layer_4_nodes], dtype=tf.float32), name=\"biases4\")\n",
    "\n",
    "layer_4_output = tf.matmul(layer_3_output, w4) + b4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choices of loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = layer_4_output\n",
    "cost = tf.losses.softmax_cross_entropy(logits=prediction, onehot_labels=Y)\n",
    "# cost = tf.reduce_mean(tf.squared_difference(prediction, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.16765\n",
      "The current training cost: 2.7198617458343506\n",
      "\n",
      "At iteration 20, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.7591\n",
      "The current training cost: 0.7878494262695312\n",
      "\n",
      "At iteration 40, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.8507\n",
      "The current training cost: 0.5017147064208984\n",
      "\n",
      "At iteration 60, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.88435\n",
      "The current training cost: 0.39759373664855957\n",
      "\n",
      "At iteration 80, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9013\n",
      "The current training cost: 0.334685742855072\n",
      "\n",
      "At iteration 100, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.91465\n",
      "The current training cost: 0.2925950884819031\n",
      "\n",
      "At iteration 120, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.92365\n",
      "The current training cost: 0.26139533519744873\n",
      "\n",
      "At iteration 140, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.93145\n",
      "The current training cost: 0.2366476058959961\n",
      "\n",
      "At iteration 160, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9379\n",
      "The current training cost: 0.21623146533966064\n",
      "\n",
      "At iteration 180, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.94275\n",
      "The current training cost: 0.19907450675964355\n",
      "\n",
      "At iteration 200, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9475\n",
      "The current training cost: 0.1844331920146942\n",
      "\n",
      "At iteration 220, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9513\n",
      "The current training cost: 0.1716948300600052\n",
      "\n",
      "At iteration 240, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.95495\n",
      "The current training cost: 0.1604653298854828\n",
      "\n",
      "At iteration 260, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9578\n",
      "The current training cost: 0.15043655037879944\n",
      "\n",
      "At iteration 280, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9611\n",
      "The current training cost: 0.14139661192893982\n",
      "\n",
      "At iteration 300, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.96395\n",
      "The current training cost: 0.1332360804080963\n",
      "\n",
      "At iteration 320, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9656\n",
      "The current training cost: 0.12579253315925598\n",
      "\n",
      "At iteration 340, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9688\n",
      "The current training cost: 0.1189558133482933\n",
      "\n",
      "At iteration 360, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.97105\n",
      "The current training cost: 0.11263427883386612\n",
      "\n",
      "At iteration 380, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.97275\n",
      "The current training cost: 0.10681630671024323\n",
      "\n",
      "At iteration 400, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.97425\n",
      "The current training cost: 0.10143613070249557\n",
      "\n",
      "At iteration 420, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.97575\n",
      "The current training cost: 0.09642747789621353\n",
      "\n",
      "At iteration 440, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.97685\n",
      "The current training cost: 0.09176286309957504\n",
      "\n",
      "At iteration 460, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9779\n",
      "The current training cost: 0.08741626888513565\n",
      "\n",
      "At iteration 480, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.97925\n",
      "The current training cost: 0.08335217088460922\n",
      "\n",
      "At iteration 500, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9813\n",
      "The current training cost: 0.07954902201890945\n",
      "\n",
      "At iteration 520, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.98245\n",
      "The current training cost: 0.07597995549440384\n",
      "\n",
      "At iteration 540, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9832\n",
      "The current training cost: 0.07261904329061508\n",
      "\n",
      "At iteration 560, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.98425\n",
      "The current training cost: 0.06944391131401062\n",
      "\n",
      "At iteration 580, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.98505\n",
      "The current training cost: 0.06644326448440552\n",
      "\n",
      "At iteration 600, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.98565\n",
      "The current training cost: 0.06360769271850586\n",
      "\n",
      "At iteration 620, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9864\n",
      "The current training cost: 0.06092924624681473\n",
      "\n",
      "At iteration 640, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9868\n",
      "The current training cost: 0.05839409679174423\n",
      "\n",
      "At iteration 660, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9875\n",
      "The current training cost: 0.05598343536257744\n",
      "\n",
      "At iteration 680, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9879\n",
      "The current training cost: 0.0536932572722435\n",
      "\n",
      "At iteration 700, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.98865\n",
      "The current training cost: 0.05151265859603882\n",
      "\n",
      "At iteration 720, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.98915\n",
      "The current training cost: 0.049439437687397\n",
      "\n",
      "At iteration 740, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.98975\n",
      "The current training cost: 0.047467589378356934\n",
      "\n",
      "At iteration 760, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9904\n",
      "The current training cost: 0.045590486377477646\n",
      "\n",
      "At iteration 780, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.99085\n",
      "The current training cost: 0.043800968676805496\n",
      "\n",
      "At iteration 800, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9915\n",
      "The current training cost: 0.0420963279902935\n",
      "\n",
      "At iteration 820, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9921\n",
      "The current training cost: 0.04047315567731857\n",
      "\n",
      "At iteration 840, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.99235\n",
      "The current training cost: 0.03892505541443825\n",
      "\n",
      "At iteration 860, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.99275\n",
      "The current training cost: 0.037448983639478683\n",
      "\n",
      "At iteration 880, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.99325\n",
      "The current training cost: 0.03604007512331009\n",
      "\n",
      "At iteration 900, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.99365\n",
      "The current training cost: 0.03469638526439667\n",
      "\n",
      "At iteration 920, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.99405\n",
      "The current training cost: 0.033412348479032516\n",
      "\n",
      "At iteration 940, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.9945\n",
      "The current training cost: 0.03218371793627739\n",
      "\n",
      "At iteration 960, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.99465\n",
      "The current training cost: 0.031009161844849586\n",
      "\n",
      "At iteration 980, the accuracy and cost are:\n",
      "The accuracy on the training set is 0.99495\n",
      "The current training cost: 0.02988538146018982\n",
      "\n",
      "Training is complete!\n",
      "Final Training cost: 0.028863009065389633\n",
      "Final Testing cost: 1.5305941104888916\n"
     ]
    }
   ],
   "source": [
    "# saver = tf.train.Saver() # for part3\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "    \n",
    "session.run(tf.global_variables_initializer())\n",
    "    \n",
    "for i in range(iterations):\n",
    "    session.run(optimizer, feed_dict={X: X_train, Y: y_train_onehot})\n",
    "        \n",
    "    if i % 20 == 0:\n",
    "        training_cost = session.run(cost, feed_dict={X: X_train, Y:y_train_onehot})\n",
    "        current_prediction = session.run(prediction, feed_dict={X: X_train, Y:y_train_onehot})\n",
    "        current_prediction = np.argmax(current_prediction, axis=1)\n",
    "        accuracy = sum(current_prediction==y_train) / len(y_train)\n",
    "        print(\"At iteration {}, the accuracy and cost are:\".format(i))\n",
    "        print(\"The accuracy on the training set is\", accuracy)\n",
    "        print(\"The current training cost: {}\\n\".format(training_cost))\n",
    "            \n",
    "# Training is now complete!\n",
    "print(\"Training is complete!\")\n",
    "\n",
    "final_training_cost = session.run(cost, feed_dict={X: X_train, Y:y_train_onehot})\n",
    "final_testing_cost = session.run(cost, feed_dict={X: X_test, Y: y_test_onehot})\n",
    "\n",
    "print(\"Final Training cost: {}\".format(final_training_cost))\n",
    "print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
    "# saver.save(session, \"./tmp/model.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is 0.9245\n"
     ]
    }
   ],
   "source": [
    "current_prediction = session.run(prediction, feed_dict={X: X_test, Y:y_test_onehot})\n",
    "current_prediction = np.argmax(current_prediction, axis=1)\n",
    "accuracy = sum(current_prediction==y_test) / len(y_test)\n",
    "print(\"The testing accuracy is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Extra)Part 3: Save \n",
    "https://www.tensorflow.org/guide/saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    saver.restore(session, \"./tmp/model.ckpt\")\n",
    "    current_prediction = session.run(prediction, feed_dict={X: X_test, Y:y_test_onehot})\n",
    "    current_prediction = np.argmax(current_prediction, axis=1)\n",
    "    accuracy = sum(current_prediction==y_test) / len(y_test)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "saver.restore(session, \"./tmp/model.ckpt\")\n",
    "current_prediction = session.run(prediction, feed_dict={X: X_test, Y:y_test_onehot})\n",
    "current_prediction = np.argmax(current_prediction, axis=1)\n",
    "accuracy = sum(current_prediction==y_test) / len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources:\n",
    "https://www.tensorflow.org/\n",
    "https://pythonprogramming.net/machine-learning-tutorial-python-introduction/\n",
    "https://learningtensorflow.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks for coming!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
